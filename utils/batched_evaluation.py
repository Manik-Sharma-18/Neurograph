"""
Batched Evaluation System for NeuroGraph
High-performance GPU-optimized evaluation with multi-sample processing
"""

import torch
import numpy as np
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import time


class BatchedEvaluator:
    """
    GPU-optimized batched evaluation system.
    
    Key features:
    - Multi-sample batch processing
    - Memory-efficient tensor reuse
    - Streaming evaluation for large datasets
    - Real-time progress monitoring
    """
    
    def __init__(
        self,
        training_context,
        batch_size: int = 16,
        device: str = 'cuda',
        verbose: bool = True
    ):
        """
        Initialize batched evaluator.
        
        Args:
            training_context: NeuroGraph training context
            batch_size: Number of samples to process simultaneously
            device: Computation device
            verbose: Enable progress reporting
        """
        self.training_context = training_context
        self.batch_size = batch_size
        self.device = device
        self.verbose = verbose
        
        # Pre-allocate tensors for efficiency
        self.tensor_cache = {}
        self.batch_stats = {
            'total_samples': 0,
            'correct_predictions': 0,
            'batch_times': [],
            'gpu_utilization': []
        }
    
    def evaluate_accuracy_batched(
        self,
        num_samples: int = 1000,
        streaming: bool = True
    ) -> Dict[str, float]:
        """
        Evaluate model accuracy using batched processing.
        
        Args:
            num_samples: Number of samples to evaluate
            streaming: Use streaming evaluation for memory efficiency
            
        Returns:
            Dictionary with accuracy metrics and performance stats
        """
        if self.verbose:
            print(f"\nðŸŽ¯ Starting Batched Evaluation")
            print(f"   ðŸ“Š Samples: {num_samples:,}")
            print(f"   ðŸ”„ Batch size: {self.batch_size}")
            print(f"   ðŸ’¾ Device: {self.device}")
            print(f"   ðŸŒŠ Streaming: {streaming}")
        
        # Set model to evaluation mode
        if hasattr(self.training_context.input_adapter, 'eval'):
            self.training_context.input_adapter.eval()
        
        # Reset statistics
        self.batch_stats = {
            'total_samples': 0,
            'correct_predictions': 0,
            'batch_times': [],
            'gpu_utilization': []
        }
        
        start_time = time.perf_counter()
        dataset_size = self.training_context.input_adapter.get_dataset_info()['dataset_size']
        
        # Generate sample indices
        sample_indices = np.random.choice(
            dataset_size, 
            min(num_samples, dataset_size), 
            replace=False
        )
        
        # Process in batches
        num_batches = (len(sample_indices) + self.batch_size - 1) // self.batch_size
        
        with torch.no_grad():
            for batch_idx in range(num_batches):
                batch_start_idx = batch_idx * self.batch_size
                batch_end_idx = min(batch_start_idx + self.batch_size, len(sample_indices))
                batch_indices = sample_indices[batch_start_idx:batch_end_idx]
                
                # Process batch
                batch_accuracy = self._process_evaluation_batch(batch_indices, batch_idx, num_batches)
                
                # Update progress
                if self.verbose and (batch_idx + 1) % max(1, num_batches // 10) == 0:
                    elapsed = time.perf_counter() - start_time
                    samples_processed = batch_end_idx
                    samples_per_sec = samples_processed / elapsed
                    eta = (len(sample_indices) - samples_processed) / samples_per_sec
                    
                    current_accuracy = self.batch_stats['correct_predictions'] / self.batch_stats['total_samples']
                    print(f"   Batch {batch_idx+1:3d}/{num_batches}: "
                          f"Acc={current_accuracy:.1%}, "
                          f"Speed={samples_per_sec:.1f} samples/s, "
                          f"ETA={eta:.1f}s")
        
        # Calculate final metrics
        total_time = time.perf_counter() - start_time
        final_accuracy = self.batch_stats['correct_predictions'] / self.batch_stats['total_samples']
        avg_batch_time = np.mean(self.batch_stats['batch_times']) if self.batch_stats['batch_times'] else 0
        samples_per_second = self.batch_stats['total_samples'] / total_time
        
        results = {
            'accuracy': final_accuracy,
            'total_samples': self.batch_stats['total_samples'],
            'correct_predictions': self.batch_stats['correct_predictions'],
            'total_time': total_time,
            'samples_per_second': samples_per_second,
            'avg_batch_time': avg_batch_time,
            'num_batches': num_batches,
            'batch_size': self.batch_size
        }
        
        if self.verbose:
            print(f"\nâœ… Batched Evaluation Completed")
            print(f"   ðŸŽ¯ Final Accuracy: {final_accuracy:.1%}")
            print(f"   âš¡ Speed: {samples_per_second:.1f} samples/second")
            print(f"   â±ï¸  Total Time: {total_time:.1f}s")
            print(f"   ðŸ“Š Batch Performance: {avg_batch_time*1000:.1f}ms/batch")
        
        return results
    
    def _process_evaluation_batch(
        self,
        batch_indices: np.ndarray,
        batch_idx: int,
        total_batches: int
    ) -> float:
        """
        Process a single evaluation batch.
        
        Args:
            batch_indices: Sample indices for this batch
            batch_idx: Current batch index
            total_batches: Total number of batches
            
        Returns:
            Batch accuracy
        """
        batch_start_time = time.perf_counter()
        batch_correct = 0
        batch_total = 0
        
        # Pre-allocate batch tensors
        batch_size = len(batch_indices)
        input_contexts = []
        target_labels = []
        
        # Prepare batch inputs
        for sample_idx in batch_indices:
            try:
                input_context, target_label = self.training_context.input_adapter.get_input_context(
                    sample_idx, self.training_context.input_nodes
                )
                input_contexts.append(input_context)
                target_labels.append(target_label)
            except Exception as e:
                # Skip problematic samples
                continue
        
        if not input_contexts:
            return 0.0
        
        # Process forward passes (can be further optimized for true batch processing)
        predictions = []
        for i, (input_context, target_label) in enumerate(zip(input_contexts, target_labels)):
            try:
                # Forward pass
                output_signals = self.training_context.forward_pass(input_context)
                
                if output_signals:
                    # Compute prediction
                    class_encodings = self.training_context.class_encoder.get_all_encodings()
                    logits = self.training_context.loss_function.compute_logits_from_signals(
                        output_signals, class_encodings, self.training_context.lookup_tables
                    )
                    
                    predicted_class = torch.argmax(logits).item()
                    predictions.append(predicted_class)
                    
                    # Check correctness
                    if predicted_class == target_label:
                        batch_correct += 1
                else:
                    # No output signals - random prediction
                    predictions.append(np.random.randint(0, 10))
                
                batch_total += 1
                
            except Exception as e:
                # Skip problematic samples
                predictions.append(-1)  # Invalid prediction
                batch_total += 1
                continue
        
        # Update statistics
        batch_time = time.perf_counter() - batch_start_time
        self.batch_stats['total_samples'] += batch_total
        self.batch_stats['correct_predictions'] += batch_correct
        self.batch_stats['batch_times'].append(batch_time)
        
        batch_accuracy = batch_correct / batch_total if batch_total > 0 else 0.0
        return batch_accuracy
    
    def evaluate_with_detailed_metrics(
        self,
        num_samples: int = 1000
    ) -> Dict[str, any]:
        """
        Evaluate with detailed per-class metrics.
        
        Args:
            num_samples: Number of samples to evaluate
            
        Returns:
            Detailed evaluation metrics
        """
        if self.verbose:
            print(f"\nðŸ“Š Detailed Evaluation (Per-Class Metrics)")
        
        # Initialize per-class tracking
        num_classes = self.training_context.config.get('class_encoding.num_classes', 10)
        class_correct = np.zeros(num_classes)
        class_total = np.zeros(num_classes)
        confusion_matrix = np.zeros((num_classes, num_classes))
        
        # Set model to evaluation mode
        if hasattr(self.training_context.input_adapter, 'eval'):
            self.training_context.input_adapter.eval()
        
        dataset_size = self.training_context.input_adapter.get_dataset_info()['dataset_size']
        sample_indices = np.random.choice(
            dataset_size, 
            min(num_samples, dataset_size), 
            replace=False
        )
        
        start_time = time.perf_counter()
        
        with torch.no_grad():
            for i, sample_idx in enumerate(sample_indices):
                try:
                    # Get input and forward pass
                    input_context, target_label = self.training_context.input_adapter.get_input_context(
                        sample_idx, self.training_context.input_nodes
                    )
                    
                    output_signals = self.training_context.forward_pass(input_context)
                    
                    if output_signals:
                        # Compute prediction
                        class_encodings = self.training_context.class_encoder.get_all_encodings()
                        logits = self.training_context.loss_function.compute_logits_from_signals(
                            output_signals, class_encodings, self.training_context.lookup_tables
                        )
                        
                        predicted_class = torch.argmax(logits).item()
                    else:
                        # No output - random prediction
                        predicted_class = np.random.randint(0, num_classes)
                    
                    # Update per-class statistics
                    if 0 <= target_label < num_classes:
                        class_total[target_label] += 1
                        if predicted_class == target_label:
                            class_correct[target_label] += 1
                        
                        # Update confusion matrix
                        if 0 <= predicted_class < num_classes:
                            confusion_matrix[target_label, predicted_class] += 1
                    
                    # Progress reporting
                    if self.verbose and (i + 1) % max(1, len(sample_indices) // 10) == 0:
                        elapsed = time.perf_counter() - start_time
                        samples_per_sec = (i + 1) / elapsed
                        eta = (len(sample_indices) - i - 1) / samples_per_sec
                        current_acc = np.sum(class_correct) / np.sum(class_total)
                        print(f"   Sample {i+1:4d}/{len(sample_indices)}: "
                              f"Acc={current_acc:.1%}, "
                              f"Speed={samples_per_sec:.1f}/s, "
                              f"ETA={eta:.1f}s")
                
                except Exception as e:
                    # Skip problematic samples
                    continue
        
        # Calculate metrics
        total_time = time.perf_counter() - start_time
        overall_accuracy = np.sum(class_correct) / np.sum(class_total)
        per_class_accuracy = class_correct / np.maximum(class_total, 1)
        
        # Calculate precision, recall, F1 for each class
        precision = np.zeros(num_classes)
        recall = np.zeros(num_classes)
        f1_score = np.zeros(num_classes)
        
        for class_id in range(num_classes):
            tp = confusion_matrix[class_id, class_id]
            fp = np.sum(confusion_matrix[:, class_id]) - tp
            fn = np.sum(confusion_matrix[class_id, :]) - tp
            
            precision[class_id] = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall[class_id] = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1_score[class_id] = 2 * precision[class_id] * recall[class_id] / (precision[class_id] + recall[class_id]) if (precision[class_id] + recall[class_id]) > 0 else 0
        
        results = {
            'overall_accuracy': overall_accuracy,
            'per_class_accuracy': per_class_accuracy.tolist(),
            'precision': precision.tolist(),
            'recall': recall.tolist(),
            'f1_score': f1_score.tolist(),
            'confusion_matrix': confusion_matrix.tolist(),
            'class_totals': class_total.tolist(),
            'total_samples': int(np.sum(class_total)),
            'total_time': total_time,
            'samples_per_second': np.sum(class_total) / total_time
        }
        
        if self.verbose:
            print(f"\nðŸ“ˆ Detailed Evaluation Results")
            print(f"   ðŸŽ¯ Overall Accuracy: {overall_accuracy:.1%}")
            print(f"   ðŸ“Š Per-Class Accuracy: {np.mean(per_class_accuracy):.1%} (avg)")
            print(f"   ðŸŽ¯ Precision: {np.mean(precision):.1%} (avg)")
            print(f"   ðŸŽ¯ Recall: {np.mean(recall):.1%} (avg)")
            print(f"   ðŸŽ¯ F1-Score: {np.mean(f1_score):.1%} (avg)")
            print(f"   âš¡ Speed: {results['samples_per_second']:.1f} samples/second")
        
        return results
    
    def benchmark_evaluation_performance(
        self,
        batch_sizes: List[int] = [1, 4, 8, 16, 32],
        num_samples: int = 100
    ) -> Dict[int, Dict[str, float]]:
        """
        Benchmark evaluation performance across different batch sizes.
        
        Args:
            batch_sizes: List of batch sizes to test
            num_samples: Number of samples per benchmark
            
        Returns:
            Performance results for each batch size
        """
        if self.verbose:
            print(f"\nâš¡ Benchmarking Evaluation Performance")
            print(f"   ðŸ“Š Batch sizes: {batch_sizes}")
            print(f"   ðŸ”¢ Samples per test: {num_samples}")
        
        results = {}
        original_batch_size = self.batch_size
        
        for batch_size in batch_sizes:
            if self.verbose:
                print(f"\n   Testing batch size: {batch_size}")
            
            # Update batch size
            self.batch_size = batch_size
            
            # Run benchmark
            start_time = time.perf_counter()
            eval_results = self.evaluate_accuracy_batched(
                num_samples=num_samples,
                streaming=True
            )
            benchmark_time = time.perf_counter() - start_time
            
            results[batch_size] = {
                'accuracy': eval_results['accuracy'],
                'total_time': benchmark_time,
                'samples_per_second': eval_results['samples_per_second'],
                'avg_batch_time': eval_results['avg_batch_time'],
                'memory_efficiency': batch_size / benchmark_time  # Simple metric
            }
            
            if self.verbose:
                print(f"     âœ… Accuracy: {eval_results['accuracy']:.1%}")
                print(f"     âš¡ Speed: {eval_results['samples_per_second']:.1f} samples/s")
                print(f"     â±ï¸  Time: {benchmark_time:.1f}s")
        
        # Restore original batch size
        self.batch_size = original_batch_size
        
        # Find optimal batch size
        optimal_batch_size = max(results.keys(), key=lambda k: results[k]['samples_per_second'])
        
        if self.verbose:
            print(f"\nðŸ† Optimal Batch Size: {optimal_batch_size}")
            print(f"   âš¡ Best Speed: {results[optimal_batch_size]['samples_per_second']:.1f} samples/s")
        
        return results
    
    def get_evaluation_stats(self) -> Dict[str, any]:
        """Get current evaluation statistics."""
        return self.batch_stats.copy()
    
    def reset_stats(self):
        """Reset evaluation statistics."""
        self.batch_stats = {
            'total_samples': 0,
            'correct_predictions': 0,
            'batch_times': [],
            'gpu_utilization': []
        }


# Factory function
def create_batched_evaluator(
    training_context,
    batch_size: int = 16,
    device: str = 'cuda',
    verbose: bool = True
) -> BatchedEvaluator:
    """
    Create a batched evaluator for the training context.
    
    Args:
        training_context: NeuroGraph training context
        batch_size: Evaluation batch size
        device: Computation device
        verbose: Enable progress reporting
        
    Returns:
        Configured batched evaluator
    """
    return BatchedEvaluator(training_context, batch_size, device, verbose)
